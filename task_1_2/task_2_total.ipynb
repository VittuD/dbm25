{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f302d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import grayscale_moments\n",
    "import resnet\n",
    "import hog\n",
    "import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_path = '../data/Part1/Part1'\n",
    "data2_path = '/workspaces/dbm25/data/Part2/Part2'\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c80a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel, ViTImageProcessor\n",
    "\n",
    "# 1. Load the ViT config from your local folder\n",
    "vision_config = ViTConfig.from_pretrained(\n",
    "    \"/workspaces/dbm25/data/vit_b16_224-mluke/vision_encoder\"\n",
    ")\n",
    "\n",
    "# 2. Load the model weights into a ViTModel\n",
    "model = ViTModel.from_pretrained(\n",
    "    \"/workspaces/dbm25/data/vit_b16_224-mluke/vision_encoder\",\n",
    "    config=vision_config,\n",
    "    # map_location=\"cpu\",  # uncomment if you need CPU-only\n",
    ")\n",
    "\n",
    "# 3. Set to eval mode if youâ€™re doing inference\n",
    "model.eval()\n",
    "\n",
    "# 4. Prepare your images for ViT\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# If you have PIL images in a list called `images`:\n",
    "inputs = processor(images=rgb, return_tensors=\"pt\")\n",
    "\n",
    "outputs   = model(**inputs)\n",
    "# Print the keys of the output and their shapes\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a55f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = []\n",
    "# For each image in data1_path extract the features\n",
    "for root, dirs, files in os.walk(data2_path):\n",
    "    for image in files:\n",
    "        if not image.endswith('.jpg'):\n",
    "            continue\n",
    "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "        model.eval()    \n",
    "        # Class can be glioma, menin, tumor. Get the class from the filename\n",
    "        class_name = image.split('_')[0]+'_' + image.split('_')[1]\n",
    "        image_path = os.path.join(f'{class_name}/', image)\n",
    "        # Get the full path to the image\n",
    "        image_path = os.path.abspath(os.path.join(data1_path, image_path))\n",
    "        print(f'Image Path {image_path}')\n",
    "        features_dict = extract_features.extract_features(image_path, model)\n",
    "        \n",
    "        # Append the features to the list\n",
    "        extracted_features.append(features_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted features into a pt file\n",
    "torch.save(extracted_features, '../data/extracted_features_part_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dcbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted features from the pt file\n",
    "extracted_features_loaded = torch.load('../data/extracted_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f491f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_test_features = extract_features.extract_features(f'{data2_path}/brain_glioma/brain_glioma_1962.jpg', model)\n",
    "print(part2_test_features)\n",
    "avgpool_test = part2_test_features['layer3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f584e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_elem = extracted_features[0]\n",
    "# Print the shape of each key in the dictionary\n",
    "for key in first_elem.keys():\n",
    "    # Check if it's a tensor\n",
    "    if isinstance(first_elem[key], torch.Tensor):\n",
    "        print(f'{key}: {first_elem[key].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_elem_loaded = extracted_features_loaded[0]\n",
    "# Print the shape of each key in the dictionary\n",
    "for key in first_elem_loaded.keys():\n",
    "    # Check if it's a tensor\n",
    "    if isinstance(first_elem_loaded[key], torch.Tensor):\n",
    "        print(f'{key}: {first_elem_loaded[key].shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
