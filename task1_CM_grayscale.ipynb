{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f822e7a-2d71-452a-89fe-b181bbdf2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44965b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_path = 'data/Part1/Part1'\n",
    "data2_path = 'data/Part2/Part2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb1ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pretrained ResNet-50 model from torchvision\n",
    "# and load it into the model.\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee749c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image\n",
    "def resnet_preprocessor(image):\n",
    "    # Resize the image to 224x224\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # Convert the image to a tensor\n",
    "    image = torch.tensor(image).permute(2, 0, 1).float()\n",
    "    # Normalize the image\n",
    "    image = (image - 127.5) / 127.5\n",
    "    # Add a batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449f66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_features(\n",
    "    model: torch.nn.Module,\n",
    "    x: torch.Tensor,\n",
    "    layer_name: str\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run x through model, grab the output of a specified layer via forward hook,\n",
    "    then remove the hook and return the raw feature tensor.\n",
    "\n",
    "    Args:\n",
    "        model:        A PyTorch model (e.g. torchvision.models.resnet50(pretrained=True))\n",
    "        x:            Input tensor of shape [B, C, H, W] (already preprocessed)\n",
    "        layer_name:   Dot-separated layer name as in model.named_modules()\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Output of the specified layer\n",
    "    \"\"\"\n",
    "    # Store hooked output\n",
    "    features = {}\n",
    "\n",
    "    def _hook(module, inp, out):\n",
    "        # Detach and store output\n",
    "        features['feat'] = out.detach()\n",
    "\n",
    "    # Find modules by name\n",
    "    modules = dict(model.named_modules())\n",
    "    if layer_name not in modules:\n",
    "        raise ValueError(f\"Layer '{layer_name}' not found in model.\")\n",
    "    \n",
    "    # Register hook\n",
    "    handle = modules[layer_name].register_forward_hook(_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "\n",
    "    # Remove hook\n",
    "    handle.remove()\n",
    "\n",
    "    # Return raw features\n",
    "    return features.get('feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c411e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 714\n"
     ]
    }
   ],
   "source": [
    "# Open image using OpenCV from data/Part1/Part1/brain_glioma/brain_glioma_0001.jpg\n",
    "image = cv2.imread(f'{data1_path}/brain_glioma/brain_glioma_0001.jpg')\n",
    "# Pass the image through the model and print the classification\n",
    "image = resnet_preprocessor(image)\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    output = model(image)  # Forward pass\n",
    "# Get the predicted class\n",
    "_, predicted = torch.max(output, 1)\n",
    "# Print the predicted class\n",
    "print(f'Predicted class: {predicted.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d4a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([1, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Open image using OpenCV from data/Part1/Part1/brain_glioma/brain_glioma_0001.jpg\n",
    "image = cv2.imread(f'{data1_path}/brain_glioma/brain_glioma_0001.jpg')\n",
    "# Pass the image through the model and print the classification\n",
    "image = resnet_preprocessor(image)\n",
    "# Extract features from the 'avgpool' layer\n",
    "features = extract_layer_features(model, image, 'avgpool')\n",
    "# Print the shape of the features\n",
    "print(f'Features shape: {features.shape}')  # e.g. torch.Size([1, 2048, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b936e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([1, 1024, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Open image using OpenCV from data/Part1/Part1/brain_glioma/brain_glioma_0001.jpg\n",
    "image = cv2.imread(f'{data1_path}/brain_glioma/brain_glioma_0001.jpg')\n",
    "# Pass the image through the model and print the classification\n",
    "image = resnet_preprocessor(image)\n",
    "# Extract features from the 'avgpool' layer\n",
    "features = extract_layer_features(model, image, 'layer3')\n",
    "# Print the shape of the features\n",
    "print(f'Features shape: {features.shape}')  # e.g. torch.Size([1, 2048, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9dfb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Open image using OpenCV from data/Part1/Part1/brain_glioma/brain_glioma_0001.jpg\n",
    "image = cv2.imread(f'{data1_path}/brain_glioma/brain_glioma_0001.jpg')\n",
    "# Pass the image through the model and print the classification\n",
    "image = resnet_preprocessor(image)\n",
    "# Extract features from the 'avgpool' layer\n",
    "features = extract_layer_features(model, image, 'fc')\n",
    "# Print the shape of the features\n",
    "print(f'Features shape: {features.shape}')  # e.g. torch.Size([1, 2048, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d54969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (10, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open image using OpenCV from data/Part1/Part1/brain_glioma/brain_glioma_0001.jpg\n",
    "image = cv2.imread(f'{data1_path}/brain_glioma/brain_glioma_0001.jpg')\n",
    "image = image[:, :, 0]\n",
    "# Resize to 10*10\n",
    "image = cv2.resize(image, (10, 10))\n",
    "\n",
    "print(f'Image shape: {image.shape}')\n",
    "\n",
    "# Show image using matplotlib\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5151224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks: 100\n",
      "Block shape: (30, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open image using OpenCV from data/Part1/Part1/brain_glioma/brain_glioma_0001.jpg\n",
    "image = cv2.imread(f'{data1_path}/brain_glioma/brain_glioma_0002.jpg')\n",
    "# Convert to grayscale\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Resize the image to 300x100\n",
    "image = cv2.resize(image, (300, 100))\n",
    "# Partition the image into 10*10 blocks\n",
    "blocks = []\n",
    "# Divide the image into non-overlapping 30*10 blocks\n",
    "for i in range(0, image.shape[0], 10):\n",
    "    for j in range(0, image.shape[1], 30):\n",
    "        block = image[i:i+10, j:j+30].T  # Transpose the block to get shape (30, 10)\n",
    "        blocks.append(block)\n",
    "\n",
    "print(f'Number of blocks: {len(blocks)}')\n",
    "print(f'Block shape: {blocks[99].shape}')\n",
    "\n",
    "# Show the 100th block using matplotlib\n",
    "plt.imshow(blocks[99], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4263717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(image):\n",
    "    \"\"\"\n",
    "    Calculate the first 3 moments of the image, mean, variance, and skewness.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Moments of the image\n",
    "    \"\"\"\n",
    "    # Calculate the mean\n",
    "    mean = np.mean(image)\n",
    "    # Calculate the variance\n",
    "    variance = np.var(image)\n",
    "    # Calculate the skewness\n",
    "    skewness = np.mean((image - mean) ** 3) / (variance ** 1.5) if variance != 0 else 0\n",
    "    return np.array([mean, variance, skewness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ba317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "moments = []\n",
    "# Compute moments for all blocks\n",
    "for i, block in enumerate(blocks):\n",
    "    # Compute the moments of the block\n",
    "    block_moments = get_moments(block)\n",
    "    # Append the moments to the list\n",
    "    moments.append(block_moments)\n",
    "# Convert moments to a numpy array\n",
    "moments = np.array(moments)\n",
    "print(moments.shape)\n",
    "print(moments[99])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
